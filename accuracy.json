{
    "model type": {
        "type": "flash attention"
    },
    "h-params": {
        "batch_size": 64,
        "block_size": 256,
        "dropout": 0.3,
        "n_embeddings": 384,
        "nhead": 4,
        "num_decoder_layers": 4,
        "learning_rate": 0.0001,
        "max_epochs": 5000,
        "eval_interval": 500,
        "vocab_size": 65,
        "train_val_split": 0.9,
        "device": "cuda"
    },
    "inference time": {
<<<<<<< HEAD
        "time": "44639.1796875"
=======
        "time": "27911.763671875"
>>>>>>> a3123ed (initial results)
    },
    "model params": {
        "params": "9011777"
    },
    "fp32 ram usage": {
<<<<<<< HEAD
        "ram": "47.461376"
    },
    "loss test": {
        "val": "tensor(1.5760)"
=======
        "ram": "57.684992"
    },
    "loss test": {
        "val": "tensor(1.6083)"
>>>>>>> a3123ed (initial results)
    }
}